%==== PACKAGES AND OTHER DOCUMENT CONFIGURATIONS  ====%

\documentclass{resume} % Use the custom resume.cls style

\usepackage[left=0.25in,top=0.25in,right=0.25in,bottom=0.25in]{geometry} % Document margins
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{fontawesome} % For GitHub and LinkedIn symbols
\usepackage{textcomp} % For mobile phone and email symbols
% \usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{xcolor}  % Required for defining custom colors
\usepackage{hyperref}
% Define your custom colors
% \definecolor{myblue}{RGB}{173, 216, 246}
% \definecolor{myblue}{RGB}{123, 176, 206}
\definecolor{myblue}{RGB}{93, 142, 172}


% Set hyperlink colors
\hypersetup{
    colorlinks=true,
    linkcolor=myblue,
    citecolor=myblue,
    urlcolor=myblue
}


\usepackage{hyperref}

\name{\VAR{name}} % Your name
% \address{ } % Your phone number and email
\address{
\BLOCK{ if personal.phone }{\faPhone} \href{tel:\VAR{personal.phone}}{\VAR{personal.phone}} \quad \BLOCK{ endif }
\BLOCK{ if personal.email }{\faEnvelope} \href{mailto:\VAR{personal.email}}{\VAR{personal.email}} \quad \BLOCK{ endif }
\BLOCK{ if personal.github }{\faGithub} \href{\VAR{personal.github}}{\VAR{personal.github}} \quad \BLOCK{ endif }
\BLOCK{ if personal.linkedin }{\faLinkedin} \href{\VAR{personal.linkedin}}{\VAR{personal.linkedin}} \BLOCK{ endif }
% \href{https://ameyportfolio.netlify.app/}{ameybhilegaonkar.com}/
}

\begin{document}
% \quad \quad \quad With over \textbf{3 years} of expertise in Designing and Managing \textbf{ETL Data Pipelines for Distributed Database systems}
%==== EDUCATION SECTION ====%

\begin{rSection}{Education}
\BLOCK{ for school in education }
\BLOCK{ if school.university }
\textbf{\VAR{school.university}} \hfill {\VAR{school.from} - \VAR{school.to}} \\ 
% \href{https://drive.google.com/file/d/1CBoVt4rgM3gzX-zR2J2ZyE9mspKE_VHk/view?usp=share_link}
\BLOCK{ if school.degree } {\VAR{school.degree}} \BLOCK{ endif }  {(GPA: \VAR{school.grade})} \smallskip \\ 
\textbf{\textit{Relevant Courses:}}
% \textit{Data Processing at Scale, Data Mining, Social Media Mining, Semantic Web Mining, Data Visualization, Statistical Machine Learning, Spatial Data Science}
\BLOCK{ for course in school.coursework }
\textit{\VAR{course}},
\BLOCK{ endfor }
\vspace{2mm}
\BLOCK{ endif } 
\BLOCK{ endfor }
\end{rSection}

%==== TECHNICAL STRENGTHS SECTION ====%

\begin{rSection}{Technical Skills}

\begin{tabular}{ @{} >{\bfseries}l @{\hspace{1ex}} l }
%==== programming languages ====%
Programming Languages & Python, Java, SQL, Unix / Linux Scripting\\
% \newcommand\myfontsize{\fontsize{.1pt}{.1pt}\selectfont} \myfontsize \color{white}{Cassandra, Agile, Redis, Azure, R, Go, C\#, open source community, Cassandra, Redis, Memcached, MATLAB, Object-oriented programming, automated driving, robotics, dynamics and controls, Microsoft, high-performance computing, supervised learning, unsupervised, twilio} \\

%==== Cloud platforms ====%
Cloud Platforms \& Databases & GCP, AWS, Big Query, Cassandra, Postgres, DynamoDB, MySQL \\
% \newcommand\myfontsize{\fontsize{.1pt}{.1pt}\selectfont} \myfontsize \color{white}{Hadoop, MapReduce, Kotlin, Swift, Objective C, Android, iPhone, and iPad, Jira, GSuite, Scrum, Kanban, Naïve Bayes, SVM, NN, Boosting Methods, Spark, ICML, NeurIPS, Apache Kafka, Apache Samza, Pinot, Espresso, Ambry, Helix, AWS Lambda, AWS Step Functions, AWS ElasticCache, SNS, SQS, cost-benefit, forecasting, experiment analysis, oracle, Helix, AWS Lambda, AWS Step Functions, AWS ElasticCache, SNS, SQS, cost-benefit, forecasting, experiment analysis, oracle}\\

% ==== Libraries and Frameworks ====
% Libraries \& Frameworks & Git, Spring Boot, Keras, Scikit Learn, PyTorch\\
% \newcommand\myfontsize{\fontsize{.1pt}{.1pt}\selectfont} \myfontsize \color{white}{DNS, HTTP, TCP, UDP, Socket Programming, Network, Protocol, Routing}\\

% ==== Libraries and Frameworks ====
Data Engineering & Spark, Kafka, SnowFlake, Airflow, Tableau, ETL, Pandas, D3.js

% ==== EXTRA SKILLS ====
\newcommand\myfontsize{\fontsize{.1pt}{.1pt}\selectfont} \myfontsize \color{white}
% {Advanced working SQL knowledge, Big Query, query authoring (SQL), relational databases, Hadoop, Spark, GCS, GCP cloud services (GCS, Dataproc), stream-processing systems (Storm, Spark-Streaming), Python, Scala, ‘big data, GCP 'big data' technologies,}
{EMR, EC2, S3, GLUE, KMS, Lambda, DynamoDB, DevOps, Site Reliability Engineering, Continuous Integration, Continuous Deployment, Continuous Delivery, Infrastructure as Code (IaC), Automation, Configuration Management, Microservices, Containerization, Docker, Kubernetes, Orchestration, Scalability, Availability, Monitoring, Alerting, Incident Management, Change Management, Version Control, Git, Jenkins, CI/CD Pipelines, Deployment Automation, Infrastructure Automation, Cloud Computing, AWS (Amazon Web Services)}\\
% {, Azure, Google Cloud Platform (GCP), Hybrid Cloud, Serverless Computing, Load Balancing, High Availability, Disaster Recovery, Performance Optimization, Security Best Practices, Compliance, DevSecOps, Infrastructure Monitoring, Application Performance Monitoring (APM), Infrastructure Resilience, Chaos Engineering, Incident Response, On-Call Support, Service Level Objectives (SLOs), Service Level Agreements (SLAs), Capacity Planning, Infrastructure Scaling, Infrastructure Cost Management, Log Management. }\\
% ==== EXTRA SKILLS END ====

% ==== DevOps / SRE / Jenkins  ====
DevOps / SRE & CI/CD, Git, Jenkins, Docker, Kubernetes, EC2, ECR, S3\\
Certified Google Cloud Platform & \href{https://www.credential.net/c799d945-5e89-47a3-845e-8b97575a8342}{\textbf{Associate Cloud Engineer}.} \\
% \newcommand\myfontsize{\fontsize{.1pt}{.1pt}\selectfont} \myfontsize \color{white}{EMR, EC2, S3, GLUE, KMS, Lambda, DynamoDB, DevOps, Site Reliability Engineering, Continuous Integration, Continuous Deployment, Continuous Delivery, Infrastructure as Code (IaC), Automation, Configuration Management, Microservices, Containerization, Docker, Kubernetes, Orchestration, Scalability, Availability, Monitoring, Alerting, Incident Management, Change Management, Version Control, Git, Jenkins, CI/CD Pipelines, Deployment Automation, Infrastructure Automation, Cloud Computing, AWS (Amazon Web Services), Azure, Google Cloud Platform (GCP), Hybrid Cloud, Serverless Computing, Load Balancing, High Availability, Disaster Recovery, Performance Optimization, Security Best Practices, Compliance, DevSecOps, Infrastructure Monitoring, Application Performance Monitoring (APM), Infrastructure Resilience, Chaos Engineering, Incident Response, On-Call Support, Service Level Objectives (SLOs), Service Level Agreements (SLAs), Capacity Planning, Infrastructure Scaling, Infrastructure Cost Management, Log Management.}\\

\end{tabular}
\end{rSection}

%==== WORK EXPERIENCE SECTION ====%

\begin{rSection}{Work Experience}

\begin{rSubsection}{\href{https://www.bigcommerce.com/}{BigCommerce}}{Austin, Texas}{\normalfont{Data Science Intern}}{\normalfont{June 2023 - August 2023}}
\item Constructed a logistic regression predictive classifier machine learning model for 3 million opportunities, utilizing \textbf{SQL}, \textbf{ETL}, and \textbf{data modeling techniques}, to predict customer retention post-7-day trial with increased accuracy of 12\%.
\item Executed insightful \textbf{Exploratory Data Analysis (EDA)}, implementing effective data strategies to elevate data quality and lay a robust foundation for predictive modeling.
% \item Utilized \textbf{REST API} integration to obtain and maintain a comprehensive database of contact details from CRM tools, highlighting familiarity with full-stack development and API design.
\item Presented findings and new ideas for Hyper-personalization using AI to the director of technology, resulting in enterprise-scale adoption for driving business growth.
% \item Collaborated with stakeholders across departments, including marketing and product teams, to understand business requirements and align objectives, ensuring the model's relevance and impact on key metrics.
% \item Took ownership of deliverables, adhered to deadlines, and effectively managed project timelines, ensuring timely completion of milestones and successful project outcomes.
% \item Documented project methodologies, best practices, and findings, facilitating knowledge sharing and enabling future replication of the project's success.

\end{rSubsection}
\begin{rSubsection}{\href{https://www.publicissapient.com/}{Publicis Sapient}}{Bangalore, India}{\normalfont{Data Engineer - II}}{\normalfont{June 2019 - July 2022}}

% \item Spearheaded the design and management of Data Ingestion pipelines, orchestrating data extraction, transformation, and loading processes for handling 80 million daily transactions from diverse sources, such as \textbf{Redshift}, \textbf{S3}, \textbf{Kinesis Streams}, and \textbf{Kafka}. Proficiently utilized Java, \textbf{Spark}, and \textbf{NoSQL Cassandra}.
% % \item Transformed the outdated deployment process by implementing \textbf{automated CI/CD pipelines} with Jenkins, enabling seamless docker containerized, Batch, and Real-Time deployments with a 45\% boost in efficiency.
% % \item Demonstrated the ability to provide reliable production support and quickly resolve high-stakes issues, leveraging keen problem-solving skills to ensure optimal system performance and exceed client expectations.
% % \item Optimized \textbf{SQL} queries to extract and analyze data from large databases, cutting pipeline run-time by 2 hours weekly.
% \item Utilized \textbf{AWS Lambda, and SNS} for scheduled data transformations, cutting pipeline run-time by 2 hours weekly.
% \item Led development of Data Validation module, reducing manual efforts by over 75\% across a 15+ team.
% % \item Resolved critical issues in large-scale infrastructure markets with over 30 million daily customer transactions, resulting in a 15\% revenue increase.
% \item Incorporated efficient OOP paradigms and Design Patterns for \textbf{large-scale infrastructure} scalable to over 5M US customers. 
% \item Designed and implemented automated Data Ingestion \textbf{ETL Pipelines}, processing up to 30 million daily transactions, utilizing Python, Pandas, \textbf{Google Cloud Platform BigQuery}, and \textbf{Airflow}, reducing data processing time by 25\% and ensuring \textbf{data accuracy and security}.
% % \item Engineered scalable frameworks for key components with optimal design impacting over 10 million customers.

    \item Engineered complex \textbf{ETL} pipelines using \textbf{Apache Spark}, optimizing data \textbf{extraction, transformation, }and\textbf{ loading} from diverse sources, such as \textbf{Redshift}, \textbf{S3}, \textbf{Kinesis Streams}, and \textbf{Kafka}.
    \item Utilized distributed computing frameworks such as \textbf{Apache Spark} to manage \textbf{large-scale data processing} tasks, enhancing performance and resource utilization by 15\%.
    \item Ensured data accuracy, integrity, and consistency through robust data quality checks and automated testing frameworks.
    \item Designed and maintained \textbf{real-time data streaming} solutions with \textbf{Apache Spark, }and\textbf{ GCP Cloud Run} staying current with the latest candidate activities.
    % \item Led development of Data Validation module, reducing manual efforts by over 75\% across a 15+ team.
    \item Utilized \textbf{AWS Lambda, and SNS} for scheduled data transformations, cutting pipeline run-time by 2 hours weekly.
\end{rSubsection}

%====

\end{rSection}

%==== PROJECTS SECTION ====%

\begin{rSection}{Projects}


%==== Opportunity Hackathon ==== %
\begin{rSubsection}{\href{https://github.com/2023-opportunity-hack/SouL--DigitalRecordsManagementforMuseumsandHistoricalSites}{Search Engine for All file types - Opportunity Hackathon }}{}{}{}
    \item Spearheaded \textbf{Elasticsearch} implementation for blazing-fast search responses, with \textbf{millisecond response times}.
    \item Converted and stored every file type data as \textbf{vector embeddings}, ensuring \textbf{low-latency search} capabilities.
    \item Led \textbf{Python FAST API development}, providing efficient data access and retrieval.
    \item Used Machine Learning techniques such as BERT, OCR, ResNet50, and Image Captioning to parse Image features.
    \item \textbf{Collaborated} effectively with team members, optimizing task distribution for streamlined project completion.
\end{rSubsection}


%==== 
% Graph Based Docker and Kubernetes Module
%==== 
\begin{rSubsection}{\href{https://github.com/ameygoes/ASU_MCS/tree/master/SPRING_23_CLASSES_SEM_2/CSE_511_DATA_PROCESSING_AT_SCALE/PROJECTS}{Scalable Data Processing Pipeline - Neo4J, Docker, Kafka and Minikube}}{}{}{}
\item Designed and implemented a highly scalable and available data processing pipeline using \textbf{Kubernetes}, Kafka, Docker, \textbf{Neo4j}.
\item Orchestrated the setup of Kafka and Apache Zookeeper using Minikube, a lightweight Kubernetes implementation.
\item Streamlined data ingestion, and processing into Neo4j, applying PageRank and BFS for graph-based data exploration.
\end{rSubsection}

%==== 
% Email Module
%==== 

\begin{rSubsection}{\href{https://github.com/ameygoes/Automated_Email_HRs}{Email Automation Marketing Tool}}{}{}{}
\item Initiated and completed the development of a robust email automation project, streamlining job application outreach and networking efforts with individuals of similar interests.
\itemSkillfully employed REST API integration to acquire and manage a comprehensive contact database from CRM tools, demonstrating prowess in \textbf{full-stack development} and\textbf{ API design}.
% \item Automated personalized email communication, connecting with potential job applicants and professionals.
\end{rSubsection}


%==== 
% YouTube Analytics
%==== 
% \begin{rSubsection}{YouTube Analytics}{}{}{}
% \item Conducted in-depth analysis of a YouTube dataset to identify trends in YouTube videos, employing Python, Jupyter Notebooks, and AWS services. 
% % \item Effectively utilized AWS S3 for data storage \&  EMR for \textbf{large-scale data processing}, and QuickSight for \textbf{data visualization}. 
% \item Engineered interactive dashboards with \textbf{AWS QuickSight} for \textbf{data visualization}, facilitating data-driven decision-making and improving reporting efficiency by 30\%.
% \end{rSubsection}




% \begin{rSubsection}{\href{https://github.com/ameygoes/PlacementsMadeEasy}{Student Hiring Made Easy}}{}{}{}
% \item Spearheaded the development and deployment of an innovative student-hiring web application at PICT. \item Demonstrated proficiency in configuring \textbf{AWS EC2} instances to accommodate 200 daily requests, establishing load balancing and auto-scaling mechanisms that significantly reduced downtime by 80\% ensuring high availability.
% % \item Upgraded an outdated manual hiring and logging system and improved the hiring process for over 10,000 students and faculty members annually.
% \end{rSubsection}

% \begin{rSubsection}{\href{https://github.com/ameygoes/speech-emotion-recognition}{Speech Emotion Detection}}{}{}{}
% \item Researched and optimized existing emotion detection approaches by combining CNN and LSTM networks.
% \item Discovered emotion-affecting attributes in voice by analyzing audio signal features (MFCC, ZCR, Pitch, Chroma).
% % \item Compressed audio data using an Autoencoder technique to avoid data loss resulting in a boost in accuracy by 31\%. 
% \end{rSubsection}
\end{rSection}

% \begin{rSubsection}{\href{https://github.com/ameygoes/PlacementsMadeEasy}{Python Django API based web App}}{}{}{}
% \item Created a \textbf{REST API} based web application using Python and Django framework to streamline data-driven processes.
% \item Executed efficient database models and optimized querying and data storage for efficient data analysis.
% % \item Improved user experience and engagement by accomplishing responsive design and user-centered features.
% \end{rSubsection}   
% \end{rSection}

%==== 
% Data Science Module
% \begin{rSubsection}{\href{https://github.com/ameygoes/ASU_MCS/tree/master/FALL_22_CLASSES_SEM_1/CSE_575_STATISTICAL_MACHINE_LEARNING/PROJECTS}{Image Captioning using Deep Learning}}{}{}{}
%     \item{Implemented an image captioning model using ResNet50 and Xception, trained on the Flickr8K dataset.}
%      \item{Created a module for success verification that uses indicators like BLEU, to assess the success rate.}
% \end{rSubsection}

% \begin{rSubsection}{\href{https://github.com/ameygoes/DataEngineering-YoutubeDataAnalytics}{YouTube Analytics}}{}{}{}
% \item Analyzed the Kaggle dataset for trending YouTube videos using Python, Jupyter Notebooks, and AWS services, including Amazon S3 for data storage, Amazon EMR for big data processing, and Amazon QuickSight for data visualization.. 
% \item Created interactive dashboards using Amazon QuickSight to visualize the analysis results.
% \end{rSubsection}


% \begin{rSubsection}{\href{https://github.com/ameygoes/PlacementsMadeEasy}{Student Hiring Made Easy}}{}{}{}
% \item Led the successful development and implementation of a cutting-edge student-hiring web application at Pune Institute of Computer Technology, leveraging Python, Django, and MySQL.
% % \item Integrated the model to a discerning GUI to display the predicted coordinates of the satellite in a 3D space. 
% \item Configured the AWS EC2 instances to handle daily traffic load of 500 requests; implemented load balancing and auto-scaling groups to ensure 100\% uptime and reduce downtime by 80\%
% \item Upgraded an outdated manual hiring and logging system and improved the hiring process for over 10,000 students and faculty members annually.
% \end{rSubsection}


% \begin{rSubsection}{\href{https://github.com/ameygoes/ASU_MCS/tree/master/FALL_23_CLASSES_SEM_3/CSE_594_SPATIAL_DATA_SCIENCE}{Geospatial Data WebApp}}{}{}{}
% \item Conceptualized and Developed a user-facing web App utilizing Python, Scala, JavaScript, Apache Sedona, and DeckGL library where users can perform various Geospatial queries.
% \end{rSubsection}

% \begin{rSubsection}{\href{https://github.com/ameygoes/JavaMicroservice_ShoppingApplication}{Java Micro-Services: Shopping Application}}{}{}{}
% \item Currently, Developing an\textbf{ Micro-services based} server-side REST HTTP  web API backend for Shopping applications using important concepts such as Service Discovery, Centralized Configuration, Event-driven architecture, circuit breakers, etc.
% \item Implemented different services with Async communication using Kafka, RabbitMQ, and Sync communication with Resiliance4J
% \item Used both non-relational and relational databases for serving different microservices
% \item Stored secrets using Vault. Leveraged elastic search, logstash, and Kibana for logging
% \end{rSubsection}

% \begin{rSubsection}{\href{https://github.com/ameygoes/employee-management-system}{Full Stack Employee Management System}}{}{}{}
% \item Developed a \textbf{Client-Server API-based web framework} leveraging Spring-web, to perform CRUD operations using Spring Data JPA and MySQL driver for communicating with the database layer
% \item Developing client-side interactions using frameworks like React along with plain Javascript, tailwind CSS, and HTML, deployed Axios to interact with server-side APIs
% \item Followed Design Patterns while formulating classes and API services
% \item Made software more robust by adding Facebook and Google Authentication to the System
% \end{rSubsection}



% \begin{rSection}{CERTIFICATIONS AND CO-CURRICULARS}


% \begin{rSubsection}{}{}{}{}
% \item Certified Google Cloud Platform \href{https://www.credential.net/c799d945-5e89-47a3-845e-8b97575a8342}{\textbf{Associate Cloud Engineer}.}  
% \item Mentored and tutored at \href{https://pict.edu/}{Pune Institute of Computer Technology} and \href{https://www.publicissapient.com/}{Publicis Sapient} with Data Engineering and Cloud Fundamentals concepts to 25+Business Strategy & Operations, Analytics, Cross-functional alignment, Strategic partnerships, New initiatives, Company priorities, Day-to-day operations, Future-growth initiatives, Hardware, software, services
% students and interns.
% \item Ranked 4th on Kaggle Leaderboard at \textbf{Google India Hackathon} 2018 amongst 32 shortlisted teams to compete onsite for delivering a solution to the problem of Waste Segregation using deep learning.
% \item Bagged the \textbf{second position} among 600 students who participated in the competitive programming contest \href{https://www.interviewbit.com/pages/company-coders-bit/}{\textbf{CodersBit - Pan India Annual Coding Challenge}} 2018 that covers over 300 Colleges all over India. 
% \item Supervised students across India to contribute to open source projects by implementing complex algorithms in C++ over a three-month program, \href{https://gssoc.girlscript.tech/}{\textbf{GirlScript Summer of Code 2018}}. 
% \item Appointed as an \textbf{Organizer} and \textbf{Web developer} at \href{https://www.pictieee.in/}{PICT IEEE Students Branch} 2017, a technical unit that organizes technical and non-technical programs. Managed a team of 20 members that resulted in the participation of 3000+ students.
% \end{rSubsection}

\newcommand\myfontsize{\fontsize{0.1pt}{0.1pt}\selectfont} \myfontsize \color{white}
{Data warehouse, data modeling, data extraction, data transformation, data loading, SQL, ETL, data quality, data governance, data privacy, data visualization, data controls, privacy, security, compliance, SLA, AWS, terabyte to petabyte scale data.}

\end{document}
